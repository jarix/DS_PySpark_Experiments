{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prereqquisites\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version:  3.4.1\n"
     ]
    }
   ],
   "source": [
    "# Spark Session and Context\n",
    "spark = SparkSession.builder.master(\"local\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "print(\"Spark Version: \", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "|neighbourhood_cleansed|      room_type|bedrooms|bathrooms|number_of_reviews|price|\n",
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "|      Western Addition|Entire home/apt|     1.0|      1.0|            180.0|170.0|\n",
      "|        Bernal Heights|Entire home/apt|     2.0|      1.0|            111.0|235.0|\n",
      "|        Haight Ashbury|   Private room|     1.0|      4.0|             17.0| 65.0|\n",
      "|        Haight Ashbury|   Private room|     1.0|      4.0|              8.0| 65.0|\n",
      "|      Western Addition|Entire home/apt|     2.0|      1.5|             27.0|785.0|\n",
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "Total number of rows:  7146\n"
     ]
    }
   ],
   "source": [
    "file_path = \"data/sf-airbnb/sf-airbnb-clean.parquet/\"\n",
    "\n",
    "df_airbnb = spark.read.parquet(file_path)\n",
    "\n",
    "df_airbnb.select(\"neighbourhood_cleansed\", \"room_type\", \"bedrooms\", \"bathrooms\", \"number_of_reviews\", \"price\").show(5)\n",
    "print(\"Total number of rows: \", df_airbnb.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['host_is_superhost',\n",
       " 'cancellation_policy',\n",
       " 'instant_bookable',\n",
       " 'host_total_listings_count',\n",
       " 'neighbourhood_cleansed',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'property_type',\n",
       " 'room_type',\n",
       " 'accommodates',\n",
       " 'bathrooms',\n",
       " 'bedrooms',\n",
       " 'beds',\n",
       " 'bed_type',\n",
       " 'minimum_nights',\n",
       " 'number_of_reviews',\n",
       " 'review_scores_rating',\n",
       " 'review_scores_accuracy',\n",
       " 'review_scores_cleanliness',\n",
       " 'review_scores_checkin',\n",
       " 'review_scores_communication',\n",
       " 'review_scores_location',\n",
       " 'review_scores_value',\n",
       " 'price',\n",
       " 'bedrooms_na',\n",
       " 'bathrooms_na',\n",
       " 'beds_na',\n",
       " 'review_scores_rating_na',\n",
       " 'review_scores_accuracy_na',\n",
       " 'review_scores_cleanliness_na',\n",
       " 'review_scores_checkin_na',\n",
       " 'review_scores_communication_na',\n",
       " 'review_scores_location_na',\n",
       " 'review_scores_value_na']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airbnb.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, host_is_superhost: string, cancellation_policy: string, instant_bookable: string, host_total_listings_count: string, neighbourhood_cleansed: string, latitude: string, longitude: string, property_type: string, room_type: string, accommodates: string, bathrooms: string, bedrooms: string, beds: string, bed_type: string, minimum_nights: string, number_of_reviews: string, review_scores_rating: string, review_scores_accuracy: string, review_scores_cleanliness: string, review_scores_checkin: string, review_scores_communication: string, review_scores_location: string, review_scores_value: string, price: string, bedrooms_na: string, bathrooms_na: string, beds_na: string, review_scores_rating_na: string, review_scores_accuracy_na: string, review_scores_cleanliness_na: string, review_scores_checkin_na: string, review_scores_communication_na: string, review_scores_location_na: string, review_scores_value_na: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airbnb.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, host_is_superhost: string, cancellation_policy: string, instant_bookable: string, host_total_listings_count: string, neighbourhood_cleansed: string, latitude: string, longitude: string, property_type: string, room_type: string, accommodates: string, bathrooms: string, bedrooms: string, beds: string, bed_type: string, minimum_nights: string, number_of_reviews: string, review_scores_rating: string, review_scores_accuracy: string, review_scores_cleanliness: string, review_scores_checkin: string, review_scores_communication: string, review_scores_location: string, review_scores_value: string, price: string, bedrooms_na: string, bathrooms_na: string, beds_na: string, review_scores_rating_na: string, review_scores_accuracy_na: string, review_scores_cleanliness_na: string, review_scores_checkin_na: string, review_scores_communication_na: string, review_scores_location_na: string, review_scores_value_na: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airbnb.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set size = 5780, test set size = 1366\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = df_airbnb.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"training set size = {df_train.count()}, test set size = {df_test.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare features with Transformers\n",
    "\n",
    "Linear regression requires all the input features to be contained within a single vector in a DataFrame.\n",
    "\n",
    "Transformers in Spark take a DataFrame as input and return a new DataFrame with one or more columns appended to it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----+\n",
      "|bedrooms|features|price|\n",
      "+--------+--------+-----+\n",
      "|     1.0|   [1.0]|200.0|\n",
      "|     1.0|   [1.0]|130.0|\n",
      "|     1.0|   [1.0]| 95.0|\n",
      "|     1.0|   [1.0]|250.0|\n",
      "|     3.0|   [3.0]|250.0|\n",
      "|     1.0|   [1.0]|115.0|\n",
      "|     1.0|   [1.0]|105.0|\n",
      "|     1.0|   [1.0]| 86.0|\n",
      "|     1.0|   [1.0]|100.0|\n",
      "|     2.0|   [2.0]|220.0|\n",
      "+--------+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Experiment with a simple transformer\n",
    "vec_assembler = VectorAssembler(inputCols = [\"bedrooms\"], outputCol = \"features\")\n",
    "df_vec_train = vec_assembler.transform(df_train)\n",
    "df_vec_train.select(\"bedrooms\", \"features\", \"price\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use an Estimator to Build a model\n",
    "\n",
    "An estimator takes in a DataFrame and returns a Model. Estimators learn parameters from data, have an estimator_name.fit() method, and are eagerly evaluated (i.e., kick off Spark jobs), whereas transformers are lazily evaluated.\n",
    "\n",
    "Other examples of estimators include Imputer, DecisionTreeClassifier, and RandomForestRegressor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol = \"features\", labelCol = \"price\")\n",
    "lr_model = lr.fit(df_vec_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```lr.fit()``` returns a LinearRegressionModel (```lr_model```), which is a transformer. I.e., the output of an estimator’s ```fit()``` method is a transformer. Once the estimator has learned the parameters, the transformer can apply these parameters to new data points to generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The linear regression formula is: price = 123.676*bedrooms + 47.51\n"
     ]
    }
   ],
   "source": [
    "# Inspect learned parameters\n",
    "m = round(lr_model.coefficients[0], 3)\n",
    "b = round(lr_model.intercept, 3)\n",
    "\n",
    "print(f\"The linear regression formula is: price = {m}*bedrooms + {b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Pipeline\n",
    "\n",
    "Often data preparation pipelines will have multiple steps. The Pipeline API streamlines the process by handling managing the stages to pass the data through and takes care of the processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Create a Pipeline\n",
    "pipeline = Pipeline(stages=[vec_assembler, lr])\n",
    "pipeline_model = pipeline.fit(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline API also automatically knows which stages are transformers vs. estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run test data thru Pipeline\n",
    "df_pred = pipeline_model.transform(df_test)\n",
    "df_pred.select(\"bedrooms\", \"features\", \"price\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding\n",
    "\n",
    "To convert categorical values into numeric values, use one-hot encoding (OHE).\n",
    "\n",
    "Spark internally uses a SparseVector when the majority of the entries are 0, as is often the case after OHE, so it does not waste space storing 0 values.  More memory efficient than Pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorilcal Columns:  ['host_is_superhost', 'cancellation_policy', 'instant_bookable', 'neighbourhood_cleansed', 'property_type', 'room_type', 'bed_type']\n",
      "index_output_cols:  ['host_is_superhostIndex', 'cancellation_policyIndex', 'instant_bookableIndex', 'neighbourhood_cleansedIndex', 'property_typeIndex', 'room_typeIndex', 'bed_typeIndex']\n",
      "ohe_output_cols:  ['host_is_superhostOHE', 'cancellation_policyOHE', 'instant_bookableOHE', 'neighbourhood_cleansedOHE', 'property_typeOHE', 'room_typeOHE', 'bed_typeOHE']\n",
      "handleInvalid: how to handle invalid data (unseen or NULL values) in features and label column of string type. Options are 'skip' (filter out rows with invalid data), error (throw an error), or 'keep' (put invalid data in a special additional bucket, at index numLabels). (default: error, current: skip)\n",
      "inputCol: input column name. (undefined)\n",
      "inputCols: input column names. (current: ['host_is_superhost', 'cancellation_policy', 'instant_bookable', 'neighbourhood_cleansed', 'property_type', 'room_type', 'bed_type'])\n",
      "outputCol: output column name. (default: StringIndexer_1516b1ca06ed__output)\n",
      "outputCols: output column names. (current: ['host_is_superhostIndex', 'cancellation_policyIndex', 'instant_bookableIndex', 'neighbourhood_cleansedIndex', 'property_typeIndex', 'room_typeIndex', 'bed_typeIndex'])\n",
      "stringOrderType: How to order labels of string column. The first label after ordering is assigned an index of 0. Supported options: frequencyDesc, frequencyAsc, alphabetDesc, alphabetAsc. Default is frequencyDesc. In case of equal frequency when under frequencyDesc/Asc, the strings are further sorted alphabetically (default: frequencyDesc)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "# Convert Catecorigal values to one-hot encoded numerical values\n",
    "categorical_cols = [field for (field, dataType) in df_train.dtypes if dataType == \"string\"]\n",
    "\n",
    "print(\"Categorilcal Columns: \", categorical_cols)\n",
    "\n",
    "index_output_cols = [x + \"Index\" for x in categorical_cols]\n",
    "print(\"index_output_cols: \", index_output_cols)\n",
    "ohe_output_cols = [x + \"OHE\" for x in categorical_cols]\n",
    "print(\"ohe_output_cols: \", ohe_output_cols)\n",
    "\n",
    "string_indexer = StringIndexer(inputCols = categorical_cols,\n",
    "                               outputCols = index_output_cols,\n",
    "                               handleInvalid = \"skip\")\n",
    "\n",
    "\n",
    "ohe_encoder = OneHotEncoder(inputCols = index_output_cols,\n",
    "                            outputCols = ohe_output_cols)\n",
    "                               \n",
    "numeric_cols = [field for (field, dataType ) in df_train.dtypes \n",
    "                if ((dataType == \"double\") & (field != \"price\"))]\n",
    "\n",
    "assembler_inputs = ohe_output_cols + numeric_cols\n",
    "vec_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difficulty with the above approach is that one needs to tell StringIndexer explicitly which features should be treated as categorical features.\n",
    "\n",
    "Another approach is to use RFormula. The syntax is inspired by the R programming language. With RFormula, one provides labels and which features to include. It supports a limited subset of the R operators, including ~, ., :, +, and -.\n",
    "\n",
    "The downside of RFormula automatically combining the StringIndexer and OneHotEncoder is that one-hot encoding is not required or recommended for all algorithms. E.g., tree-based algorithms can handle categorical variables directly and one can just use the StringIndexer for the categorical features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using RFormula\n",
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "r_formula = RFormula(formula=\"price ~ .\", featuresCol=\"features\", labelCol=\"price\", handleInvalid=\"skip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do Linear Regression with all the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-------------------+\n",
      "|            features| price|         prediction|\n",
      "+--------------------+------+-------------------+\n",
      "|(98,[0,3,6,22,43,...|  85.0|  55.24365707389188|\n",
      "|(98,[0,3,6,22,43,...|  45.0| 23.357685914717877|\n",
      "|(98,[0,3,6,22,43,...|  70.0| 28.474464479034395|\n",
      "|(98,[0,3,6,12,42,...| 128.0|  -91.6079079594947|\n",
      "|(98,[0,3,6,12,43,...| 159.0|  95.05688229945372|\n",
      "|(98,[0,3,6,12,43,...| 250.0| 263.54004583288406|\n",
      "|(98,[0,3,6,11,42,...|  99.0| 152.64999694892595|\n",
      "|(98,[0,3,6,31,42,...|  95.0| 180.93447481340718|\n",
      "|(98,[0,3,6,28,42,...| 100.0|-52.840740912944966|\n",
      "|(98,[0,3,6,28,43,...|2010.0| 261.05611673671683|\n",
      "|(98,[0,3,6,33,43,...| 270.0| 164.99137394276022|\n",
      "|(98,[0,3,6,33,43,...| 500.0| 376.95687662805994|\n",
      "|(98,[0,3,6,33,42,...| 125.0|  78.23687803734356|\n",
      "|(98,[0,3,6,13,42,...| 210.0| 362.62605262643956|\n",
      "|(98,[0,3,6,18,43,...|  60.0| 107.34159362908304|\n",
      "+--------------------+------+-------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(labelCol=\"price\", featuresCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[string_indexer, ohe_encoder, vec_assembler, lr])\n",
    "\n",
    "pipeline_model = pipeline.fit(df_train)\n",
    "\n",
    "df_pred = pipeline_model.transform(df_test)\n",
    "\n",
    "df_pred.select(\"features\",\"price\",\"prediction\").show(15)\n",
    "#df_pred.select(\"features\",\"price\",\"prediction\").show(n=15, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model\n",
    "\n",
    "##### RMSE = Root Mean-Square Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE =  220.56321700343753\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Calculate RMSE\n",
    "regression_evaluator = RegressionEvaluator(\n",
    "    predictionCol = \"prediction\",\n",
    "    labelCol = \"price\",\n",
    "    metricName = \"rmse\")\n",
    "\n",
    "rmse = regression_evaluator.evaluate(df_pred)\n",
    "\n",
    "print(\"RMSE = \", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### R-squared\n",
    "\n",
    "R2 = 1 - SSres/SStot\n",
    "\n",
    "If the model perfectly predicts every data point, then your SSres = 0, making your R2 = 1. And if model predicts as well as baseline (average value) then SSres = SStot, and R2 is 0. \n",
    "\n",
    "If the model performs worse than always predicting ȳ and SSres is really large? Then R2 can actually be negative, which means the model is either really bad or something is wrong.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared =  0.16043316698848087\n"
     ]
    }
   ],
   "source": [
    "# Calculate R-squared\n",
    "r2 = regression_evaluator.setMetricName(\"r2\").evaluate(df_pred)\n",
    "\n",
    "print(\"R-squared = \", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "pipeline_path = \"/tmp/lr-pipeline-model\"\n",
    "\n",
    "pipeline_model.write().overwrite().save(pipeline_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When loading back a saved model, one needs to specify the type of model one is loading back in (e.g.,  LinearRegressionModel or a LogisticRegressionModel?). For this reason, it is recommend one always puts one's transformers/estimators into a Pipeline, so that for one can load a PipelineModel and only need to change the file path to the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading back a Pipeline model\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "\n",
    "saved_pipeline_model = PipelineModel.load(pipeline_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading, one can apply the model to new data points. However, one can’t use the weights from this model as initialization parameters for training a new model (as opposed to starting with random weights), as Spark has no concept of “warm starts.” If the dataset changes slightly, one has to retrain the entire linear regression model from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning with Tree-Based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DecisionTreeRegressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For decision trees, one doesn’t have to worry about standardizing or scaling input features, because this has no impact on the splits, but one has to be careful about how to prepare categorical features.\n",
    "\n",
    "Tree-based methods can naturally handle categorical variables. In spark.ml, one just needs to pass the categorical columns to the StringIndexer, and the decision tree can take care of the rest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare DecisionTreeRegressor\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "dt = DecisionTreeRegressor(labelCol=\"price\")\n",
    "\n",
    "# Filter for numeric columns, exclude label column \"price\"\n",
    "numeric_cols = [field for (field, dataType) in df_train.dtypes \n",
    "                if ((dataType == \"double\") & (field != \"price\"))]\n",
    "\n",
    "# Combine output of StringIndexer (defined earlies in notebook) and numeric columns \n",
    "assembler_inputs = index_output_cols + numeric_cols\n",
    "vec_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "\n",
    "# Combine stages into a pipeline\n",
    "stages = [string_indexer, vec_assembler, dt]\n",
    "pipeline = Pipeline(stages=stages)\n",
    "# Need to define MaxBins in Spark\n",
    "dt.setMaxBins(40)\n",
    "pipeline_model = pipeline.fit(df_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maxBins determines the number of bins into which continuous features are discretized, or split. This discretization step is crucial for performing distributed training. There is no maxBins parameter in scikit-learn because all of the data and the model reside on a single machine. In Spark, however, workers have all the columns of the data, but only a subset of the rows. Thus, when communicating about which features and values to split on, we need to be sure they’re all talking about the same split values, which we get from the common discretization set up at training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract Decision Tree rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeRegressionModel: uid=DecisionTreeRegressor_ff0abbfa2ac7, depth=5, numNodes=47, numFeatures=33\n",
      "  If (feature 12 <= 2.5)\n",
      "   If (feature 12 <= 1.5)\n",
      "    If (feature 5 in {1.0,2.0})\n",
      "     If (feature 4 in {0.0,1.0,3.0,5.0,9.0,10.0,11.0,13.0,14.0,16.0,18.0,24.0})\n",
      "      If (feature 3 in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0})\n",
      "       Predict: 104.23992784125075\n",
      "      Else (feature 3 not in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0})\n",
      "       Predict: 250.7111111111111\n",
      "     Else (feature 4 not in {0.0,1.0,3.0,5.0,9.0,10.0,11.0,13.0,14.0,16.0,18.0,24.0})\n",
      "      If (feature 3 in {0.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,27.0,33.0,35.0})\n",
      "       Predict: 151.94179894179894\n",
      "      Else (feature 3 not in {0.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,27.0,33.0,35.0})\n",
      "       Predict: 245.8507462686567\n",
      "    Else (feature 5 not in {1.0,2.0})\n",
      "     If (feature 3 in {1.0,5.0,6.0,7.0,8.0,9.0,11.0,13.0,15.0,16.0,17.0,19.0,22.0,23.0,24.0,25.0,26.0,28.0,29.0,30.0,33.0})\n",
      "      If (feature 3 in {5.0,8.0,13.0,15.0,16.0,19.0,22.0,23.0,24.0,25.0,28.0,30.0,33.0})\n",
      "       Predict: 131.96658097686375\n",
      "      Else (feature 3 not in {5.0,8.0,13.0,15.0,16.0,19.0,22.0,23.0,24.0,25.0,28.0,30.0,33.0})\n",
      "       Predict: 164.19959266802445\n",
      "     Else (feature 3 not in {1.0,5.0,6.0,7.0,8.0,9.0,11.0,13.0,15.0,16.0,17.0,19.0,22.0,23.0,24.0,25.0,26.0,28.0,29.0,30.0,33.0})\n",
      "      If (feature 10 <= 6.5)\n",
      "       Predict: 205.5814889336016\n",
      "      Else (feature 10 > 6.5)\n",
      "       Predict: 841.6666666666666\n",
      "   Else (feature 12 > 1.5)\n",
      "    If (feature 13 <= 4.5)\n",
      "     If (feature 3 in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,15.0,16.0,17.0,18.0,19.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,33.0,34.0})\n",
      "      If (feature 14 <= 26.5)\n",
      "       Predict: 290.8357933579336\n",
      "      Else (feature 14 > 26.5)\n",
      "       Predict: 214.04819277108433\n",
      "     Else (feature 3 not in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,15.0,16.0,17.0,18.0,19.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,33.0,34.0})\n",
      "      If (feature 14 <= 3.5)\n",
      "       Predict: 741.64\n",
      "      Else (feature 14 > 3.5)\n",
      "       Predict: 309.03921568627453\n",
      "    Else (feature 13 > 4.5)\n",
      "     If (feature 15 <= 0.5)\n",
      "      If (feature 2 in {1.0})\n",
      "       Predict: 300.0\n",
      "      Else (feature 2 not in {1.0})\n",
      "       Predict: 10000.0\n",
      "     Else (feature 15 > 0.5)\n",
      "      If (feature 3 in {1.0,4.0,5.0,7.0,8.0,19.0})\n",
      "       Predict: 222.91666666666666\n",
      "      Else (feature 3 not in {1.0,4.0,5.0,7.0,8.0,19.0})\n",
      "       Predict: 398.0\n",
      "  Else (feature 12 > 2.5)\n",
      "   If (feature 1 in {0.0,1.0,2.0,3.0,4.0})\n",
      "    If (feature 12 <= 5.5)\n",
      "     If (feature 3 in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,10.0,11.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,21.0,22.0,23.0,24.0,25.0,26.0,28.0,29.0,30.0,33.0})\n",
      "      If (feature 14 <= 7.5)\n",
      "       Predict: 493.3795620437956\n",
      "      Else (feature 14 > 7.5)\n",
      "       Predict: 296.76666666666665\n",
      "     Else (feature 3 not in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,10.0,11.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,21.0,22.0,23.0,24.0,25.0,26.0,28.0,29.0,30.0,33.0})\n",
      "      If (feature 9 <= -122.411075)\n",
      "       Predict: 722.96875\n",
      "      Else (feature 9 > -122.411075)\n",
      "       Predict: 2399.4\n",
      "    Else (feature 12 > 5.5)\n",
      "     If (feature 4 in {0.0,1.0,5.0,7.0})\n",
      "      If (feature 3 in {0.0,3.0,6.0,25.0})\n",
      "       Predict: 609.5\n",
      "      Else (feature 3 not in {0.0,3.0,6.0,25.0})\n",
      "       Predict: 1715.0\n",
      "     Else (feature 4 not in {0.0,1.0,5.0,7.0})\n",
      "      Predict: 8000.0\n",
      "   Else (feature 1 not in {0.0,1.0,2.0,3.0,4.0})\n",
      "    Predict: 8000.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree rules\n",
    "dt_model = pipeline_model.stages[-1]\n",
    "print(dt_model.toDebugString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine Most Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bedrooms</td>\n",
       "      <td>0.283406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cancellation_policyIndex</td>\n",
       "      <td>0.167893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>instant_bookableIndex</td>\n",
       "      <td>0.140081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>property_typeIndex</td>\n",
       "      <td>0.128179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>number_of_reviews</td>\n",
       "      <td>0.126233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neighbourhood_cleansedIndex</td>\n",
       "      <td>0.056200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>longitude</td>\n",
       "      <td>0.038810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>minimum_nights</td>\n",
       "      <td>0.029473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>beds</td>\n",
       "      <td>0.015218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>room_typeIndex</td>\n",
       "      <td>0.010905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>accommodates</td>\n",
       "      <td>0.003603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>host_is_superhostIndex</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>bathrooms_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>beds_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>review_scores_rating_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>review_scores_accuracy_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>review_scores_cleanliness_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>review_scores_value</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>review_scores_checkin_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>review_scores_communication_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>review_scores_location_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>bedrooms_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>review_scores_rating</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>review_scores_location</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>review_scores_communication</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>review_scores_checkin</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>review_scores_cleanliness</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>review_scores_accuracy</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bathrooms</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>latitude</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>host_total_listings_count</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bed_typeIndex</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>review_scores_value_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           feature  importance\n",
       "12                        bedrooms    0.283406\n",
       "1         cancellation_policyIndex    0.167893\n",
       "2            instant_bookableIndex    0.140081\n",
       "4               property_typeIndex    0.128179\n",
       "15               number_of_reviews    0.126233\n",
       "3      neighbourhood_cleansedIndex    0.056200\n",
       "9                        longitude    0.038810\n",
       "14                  minimum_nights    0.029473\n",
       "13                            beds    0.015218\n",
       "5                   room_typeIndex    0.010905\n",
       "10                    accommodates    0.003603\n",
       "0           host_is_superhostIndex    0.000000\n",
       "24                    bathrooms_na    0.000000\n",
       "25                         beds_na    0.000000\n",
       "26         review_scores_rating_na    0.000000\n",
       "27       review_scores_accuracy_na    0.000000\n",
       "28    review_scores_cleanliness_na    0.000000\n",
       "22             review_scores_value    0.000000\n",
       "29        review_scores_checkin_na    0.000000\n",
       "30  review_scores_communication_na    0.000000\n",
       "31       review_scores_location_na    0.000000\n",
       "23                     bedrooms_na    0.000000\n",
       "16            review_scores_rating    0.000000\n",
       "21          review_scores_location    0.000000\n",
       "20     review_scores_communication    0.000000\n",
       "19           review_scores_checkin    0.000000\n",
       "18       review_scores_cleanliness    0.000000\n",
       "17          review_scores_accuracy    0.000000\n",
       "11                       bathrooms    0.000000\n",
       "8                         latitude    0.000000\n",
       "7        host_total_listings_count    0.000000\n",
       "6                    bed_typeIndex    0.000000\n",
       "32          review_scores_value_na    0.000000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "feature_importances = pd.DataFrame(\n",
    "    list(zip(vec_assembler.getInputCols(), dt_model.featureImportances)),\n",
    "    columns=[\"feature\", \"importance\"])\n",
    "\n",
    "feature_importances.sort_values(by=\"importance\", ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests are an ensemble of decision trees with two key tweaks: \n",
    "- Bootstrapping samples by rows Bootstrapping is a technique for simulating new data by sampling with replacement from the original data. Each decision tree is trained on a different bootstrap sample of the dataset, which produces slightly different decision trees, and then aggregate their predictions. This technique is known as bootstrap aggregating, or bagging. In a typical random forest implementation, each tree samples the same number of data points with replacement from the original data set, and that number can be controlled through the subsamplingRate parameter.<br /><br />\n",
    "- Random feature selection by columns The main drawback with bagging is that the trees are all highly correlated, and thus learn similar patterns in your data. To mitigate this problem, each time one wants to make a split, one only consider a random subset of the columns (1/3 of the features for RandomForestRegressor and  for RandomForestClassifier). Due to this randomness, one typically wants each tree to be quite shallow. Each of the trees learns something different about your data set, and combining this collection of “weak” learners into an ensemble makes the forest much more robust than a single decision tree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Random Forest Regressor\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "rf = RandomForestRegressor(labelCol=\"price\", maxBins=40, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests truly demonstrate the power of distributed machine learning with Spark, as each tree can be built independently of the other trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-fold Cross-Validation\n",
    "\n",
    "Split our training data into k subsets, or “folds” (e.g., three). Then, for a given hyperparameter configuration, train the model on k–1 folds and evaluate on the remaining fold, repeating this process k times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-fold validation\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "pipeline = Pipeline(stages=[string_indexer, vec_assembler, rf])\n",
    "\n",
    "# Vary maxDepth to 2, 4, or 6 and numTrees to 10 or 100\n",
    "param_grid = (ParamGridBuilder()\n",
    "              .addGrid(rf.maxDepth, [2, 4, 6])\n",
    "              .addGrid(rf.numTrees, [10, 100])\n",
    "              .build())\n",
    "\n",
    "# EValuation Criteria\n",
    "evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# Perform Cross Validation\n",
    "cv = CrossValidator(estimator=pipeline, evaluator=evaluator,\n",
    "                    estimatorParamMaps=param_grid, numFolds=3, seed=42)\n",
    "\n",
    "cv_model = cv.fit(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({Param(parent='RandomForestRegressor_f2123dc9428f', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 2,\n",
       "   Param(parent='RandomForestRegressor_f2123dc9428f', name='numTrees', doc='Number of trees to train (>= 1).'): 10},\n",
       "  291.1822640924783),\n",
       " ({Param(parent='RandomForestRegressor_f2123dc9428f', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 2,\n",
       "   Param(parent='RandomForestRegressor_f2123dc9428f', name='numTrees', doc='Number of trees to train (>= 1).'): 100},\n",
       "  286.7714750274078),\n",
       " ({Param(parent='RandomForestRegressor_f2123dc9428f', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 4,\n",
       "   Param(parent='RandomForestRegressor_f2123dc9428f', name='numTrees', doc='Number of trees to train (>= 1).'): 10},\n",
       "  287.6963245160818),\n",
       " ({Param(parent='RandomForestRegressor_f2123dc9428f', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 4,\n",
       "   Param(parent='RandomForestRegressor_f2123dc9428f', name='numTrees', doc='Number of trees to train (>= 1).'): 100},\n",
       "  279.9927057236079),\n",
       " ({Param(parent='RandomForestRegressor_f2123dc9428f', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 6,\n",
       "   Param(parent='RandomForestRegressor_f2123dc9428f', name='numTrees', doc='Number of trees to train (>= 1).'): 10},\n",
       "  294.34810870889305),\n",
       " ({Param(parent='RandomForestRegressor_f2123dc9428f', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 6,\n",
       "   Param(parent='RandomForestRegressor_f2123dc9428f', name='numTrees', doc='Number of trees to train (>= 1).'): 100},\n",
       "  275.39862704729984)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(cv_model.getEstimatorParamMaps(), cv_model.avgMetrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add parallelism parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_model = cv.setParallelism(4).fit(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another trick, put the cross validators inside the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CrossValidator(estimator=rf,\n",
    "                    evaluator=evaluator,\n",
    "                    estimatorParamMaps=param_grid, \n",
    "                    numFolds=3, \n",
    "                    parallelism=4,\n",
    "                    seed=42)\n",
    "\n",
    "pipeline = Pipeline(stages=[string_indexer, vec_assembler, cv])\n",
    "pipeline_model = pipeline.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
