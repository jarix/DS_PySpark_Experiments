{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello Spark Sandbox, random PySpark stuff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prerequisites\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version:  3.4.1\n"
     ]
    }
   ],
   "source": [
    "# Get SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").getOrCreate()\n",
    "print(\"Spark Version: \", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop over columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+\n",
      "| id|   name| age|\n",
      "+---+-------+----+\n",
      "|  1|  Alice|  25|\n",
      "|  2|   null|  30|\n",
      "|  3|Charlie|null|\n",
      "|  4|   null|null|\n",
      "+---+-------+----+\n",
      "\n",
      "Column 'id' has 0 missing value(s).\n",
      "Column 'name' has 2 missing value(s).\n",
      "Column 'age' has 2 missing value(s).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, isnan, when, count\n",
    "\n",
    "# Sample DataFrame\n",
    "data = [\n",
    "    (1, \"Alice\", 25),\n",
    "    (2, None, 30),\n",
    "    (3, \"Charlie\", None),\n",
    "    (4, None, None)\n",
    "]\n",
    "columns = [\"id\", \"name\", \"age\"]\n",
    "df_simple = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the DataFrame\n",
    "df_simple.show()\n",
    "\n",
    "# Loop over columns and check for missing values\n",
    "for column in df_simple.columns:\n",
    "    missing_count = df_simple.select(\n",
    "        count(when(col(column).isNull() | isnan(col(column)), column)).alias(\"missing_count\")\n",
    "    ).collect()[0][\"missing_count\"]\n",
    "    print(f\"Column '{column}' has {missing_count} missing value(s).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+\n",
      "|  id|   name| age|\n",
      "+----+-------+----+\n",
      "|   1|  Alice|25.0|\n",
      "|   2|   null| NaN|\n",
      "|   3|Charlie|null|\n",
      "|   4|   null|null|\n",
      "|null|    Bob|null|\n",
      "+----+-------+----+\n",
      "\n",
      "+---+----+---+\n",
      "| id|name|age|\n",
      "+---+----+---+\n",
      "|  1|   2|  4|\n",
      "+---+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, isnan, when, count\n",
    "\n",
    "\n",
    "# Sample DataFrame\n",
    "data = [\n",
    "    (1, \"Alice\", 25.0),\n",
    "    (2, None, float('NaN')),\n",
    "    (3, \"Charlie\", None),\n",
    "    (4, None, None),\n",
    "    (None, \"Bob\", None)\n",
    "]\n",
    "columns = [\"id\", \"name\", \"age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "# Check for NULL or NaN values in all columns\n",
    "missing_counts = df.select([\n",
    "    count(when(col(c).isNull() | isnan(col(c)), c)).alias(c)\n",
    "    for c in df.columns\n",
    "])\n",
    "\n",
    "# Show the counts of missing values\n",
    "missing_counts.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop Duplicate Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1|  Alice| 25|\n",
      "|  2|    Bob| 30|\n",
      "|  3|  Alice| 25|\n",
      "|  4|    Bob| 30|\n",
      "|  5|Charlie| 35|\n",
      "+---+-------+---+\n",
      "\n",
      "DataFrame after removing duplicates based on 'name' and 'age':\n",
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1|  Alice| 25|\n",
      "|  5|Charlie| 35|\n",
      "|  2|    Bob| 30|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample DataFrame\n",
    "data = [\n",
    "    (1, \"Alice\", 25),\n",
    "    (2, \"Bob\", 30),\n",
    "    (3, \"Alice\", 25),\n",
    "    (4, \"Bob\", 30),\n",
    "    (5, \"Charlie\", 35)\n",
    "]\n",
    "columns = [\"id\", \"name\", \"age\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "# Remove duplicate rows based on the subset of columns 'name' and 'age'\n",
    "df_no_duplicates = df.dropDuplicates(subset=[\"name\", \"age\"])\n",
    "\n",
    "# Show the DataFrame after removing duplicates\n",
    "print(\"DataFrame after removing duplicates based on 'name' and 'age':\")\n",
    "df_no_duplicates.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Correclation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  x|  y|\n",
      "+---+---+\n",
      "|  1|2.0|\n",
      "|  2|3.5|\n",
      "|  3|5.0|\n",
      "|  4|7.0|\n",
      "|  5|8.5|\n",
      "+---+---+\n",
      "\n",
      "Correlation between x and y: 0.9986254289035241\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a sample DataFrame\n",
    "data = [\n",
    "    (1, 2.0),\n",
    "    (2, 3.5),\n",
    "    (3, 5.0),\n",
    "    (4, 7.0),\n",
    "    (5, 8.5)\n",
    "]\n",
    "columns = [\"x\", \"y\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "# Calculate the correlation between 'x' and 'y'\n",
    "correlation = df.corr(\"x\", \"y\")\n",
    "print(f\"Correlation between x and y: {correlation}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While corr() is straightforward for small datasets, for larger or complex datasets, you can use pyspark.ml.stat.Correlation for advanced statistical analysis (e.g., calculating correlations for multiple columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|col1|col2|col3|\n",
      "+----+----+----+\n",
      "|   1| 2.0| 3.0|\n",
      "|   2| 4.5| 5.0|\n",
      "|   3| 6.0| 7.5|\n",
      "|   4| 8.0| 9.0|\n",
      "|   5|10.5|12.0|\n",
      "+----+----+----+\n",
      "\n",
      "Correlation between col1 and col2: 0.9967441085689461\n",
      "Correlation between col1 and col3: 0.9958932064677039\n",
      "Correlation between col2 and col3: 0.9948516856149789\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample DataFrame\n",
    "data = [\n",
    "    (1, 2.0, 3.0),\n",
    "    (2, 4.5, 5.0),\n",
    "    (3, 6.0, 7.5),\n",
    "    (4, 8.0, 9.0),\n",
    "    (5, 10.5, 12.0)\n",
    "]\n",
    "columns = [\"col1\", \"col2\", \"col3\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "# Get the list of column names\n",
    "columns = df.columns\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "correlations = []\n",
    "\n",
    "# Loop over pairs of columns and calculate correlation\n",
    "for i in range(len(columns)):\n",
    "    for j in range(i + 1, len(columns)):\n",
    "        col1 = columns[i]\n",
    "        col2 = columns[j]\n",
    "        corr_value = df.corr(col1, col2)  # Calculate correlation\n",
    "        correlations.append((col1, col2, corr_value))\n",
    "\n",
    "# Show the correlations\n",
    "for col1, col2, corr_value in correlations:\n",
    "    print(f\"Correlation between {col1} and {col2}: {corr_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|     features|\n",
      "+-------------+\n",
      "|[1.0,2.0,3.0]|\n",
      "|[2.0,3.5,5.0]|\n",
      "|[3.0,5.0,7.0]|\n",
      "|[4.0,7.0,8.5]|\n",
      "+-------------+\n",
      "\n",
      "Correlation matrix:\n",
      " [[1.         0.99725651 0.9978158 ]\n",
      " [0.99725651 1.         0.99018848]\n",
      " [0.9978158  0.99018848 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Sample data as vectors\n",
    "data = [(Vectors.dense([1.0, 2.0, 3.0]),),\n",
    "        (Vectors.dense([2.0, 3.5, 5.0]),),\n",
    "        (Vectors.dense([3.0, 5.0, 7.0]),),\n",
    "        (Vectors.dense([4.0, 7.0, 8.5]),)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"features\"])\n",
    "df.show()\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = Correlation.corr(df, \"features\", \"pearson\").head()[0]\n",
    "print(\"Correlation matrix:\\n\", correlation_matrix.toArray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Correlations:\n",
      "1. Columns (24, 28) - Correlation: 0.9853\n",
      "2. Columns (0, 34) - Correlation: 0.9657\n",
      "3. Columns (0, 1) - Correlation: 0.9601\n",
      "4. Columns (5, 6) - Correlation: 0.9560\n",
      "5. Columns (20, 24) - Correlation: 0.9476\n",
      "6. Columns (23, 35) - Correlation: 0.9463\n",
      "7. Columns (7, 16) - Correlation: 0.9434\n",
      "8. Columns (3, 4) - Correlation: 0.9421\n",
      "9. Columns (10, 32) - Correlation: 0.9384\n",
      "10. Columns (2, 40) - Correlation: 0.9344\n",
      "\n",
      "Highest Correlation is between Columns 24 and 28 with a value of 0.9853\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example: Generate a random 50x50 correlation matrix\n",
    "np.random.seed(42)\n",
    "correlation_matrix = np.random.rand(50, 50)\n",
    "correlation_matrix = (correlation_matrix + correlation_matrix.T) / 2  # Make it symmetric\n",
    "np.fill_diagonal(correlation_matrix, 1)  # Fill diagonal with 1s\n",
    "\n",
    "# Step 1: Extract the upper triangular part of the matrix, excluding the diagonal\n",
    "row_indices, col_indices = np.triu_indices_from(correlation_matrix, k=1)\n",
    "upper_triangular_values = correlation_matrix[row_indices, col_indices]\n",
    "\n",
    "# Step 2: Sort correlations in descending order and find the top 10\n",
    "sorted_indices = np.argsort(upper_triangular_values)[-10:]  # Indices of top 10\n",
    "top_10_correlations = upper_triangular_values[sorted_indices]\n",
    "\n",
    "# Step 3: Map back to column pairs\n",
    "top_10_pairs = [(row_indices[i], col_indices[i], upper_triangular_values[i]) for i in sorted_indices]\n",
    "\n",
    "# Step 4: Find the highest correlation and its column pair\n",
    "highest_correlation_index = np.argmax(upper_triangular_values)\n",
    "highest_pair = (row_indices[highest_correlation_index], col_indices[highest_correlation_index])\n",
    "highest_correlation = upper_triangular_values[highest_correlation_index]\n",
    "\n",
    "# Display Results\n",
    "print(\"Top 10 Correlations:\")\n",
    "for i, (row, col, value) in enumerate(sorted(top_10_pairs, key=lambda x: -x[2]), start=1):\n",
    "    print(f\"{i}. Columns ({row}, {col}) - Correlation: {value:.4f}\")\n",
    "\n",
    "print(f\"\\nHighest Correlation is between Columns {highest_pair[0]} and {highest_pair[1]} with a value of {highest_correlation:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: 20.0, Q3: 45.0, IQR: 25.0\n",
      "Lower Bound: -17.5, Upper Bound: 82.5\n",
      "Outliers:\n",
      "+---+-------+-------+\n",
      "| id|column2|column3|\n",
      "+---+-------+-------+\n",
      "|  5|    500| 1000.0|\n",
      "|  8|    800| -500.0|\n",
      "+---+-------+-------+\n",
      "\n",
      "Non-Outliers:\n",
      "+---+-------+-------+\n",
      "| id|column2|column3|\n",
      "+---+-------+-------+\n",
      "|  1|    100|   15.0|\n",
      "|  2|    200|   20.0|\n",
      "|  3|    300|   25.0|\n",
      "|  4|    400|   30.0|\n",
      "|  6|    600|   35.0|\n",
      "|  7|    700|   40.0|\n",
      "|  9|    900|   45.0|\n",
      "| 10|   1000|   50.0|\n",
      "+---+-------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 34288)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "       ^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "\n",
    "# Sample DataFrame\n",
    "data = [\n",
    "    (1, 100, 15.0),\n",
    "    (2, 200, 20.0),\n",
    "    (3, 300, 25.0),\n",
    "    (4, 400, 30.0),\n",
    "    (5, 500, 1000.0),  # Outlier in column3\n",
    "    (6, 600, 35.0),\n",
    "    (7, 700, 40.0),\n",
    "    (8, 800, -500.0),  # Outlier in column3\n",
    "    (9, 900, 45.0),\n",
    "    (10, 1000, 50.0)\n",
    "]\n",
    "columns = [\"id\", \"column2\", \"column3\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Calculate Q1, Q3, and IQR for a numeric column\n",
    "numeric_column = \"column3\"\n",
    "\n",
    "# Calculate Q1 and Q3\n",
    "quantiles = df.approxQuantile(numeric_column, [0.25, 0.75], 0.05)\n",
    "Q1, Q3 = quantiles\n",
    "\n",
    "# Calculate IQR\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "print(f\"Q1: {Q1}, Q3: {Q3}, IQR: {IQR}\")\n",
    "print(f\"Lower Bound: {lower_bound}, Upper Bound: {upper_bound}\")\n",
    "\n",
    "# Filter rows with outliers\n",
    "outliers = df.filter((col(numeric_column) < lower_bound) | (col(numeric_column) > upper_bound))\n",
    "non_outliers = df.filter((col(numeric_column) >= lower_bound) & (col(numeric_column) <= upper_bound))\n",
    "\n",
    "# Show results\n",
    "print(\"Outliers:\")\n",
    "outliers.show()\n",
    "\n",
    "print(\"Non-Outliers:\")\n",
    "non_outliers.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
