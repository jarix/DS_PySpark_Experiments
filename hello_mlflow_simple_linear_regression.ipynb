{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Regression with MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prereqquisites\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version:  3.4.1\n"
     ]
    }
   ],
   "source": [
    "# Spark Session and Context\n",
    "spark = SparkSession.builder.master(\"local\") \\\n",
    "        .appName(\"Simple MLFlow Linear Regression\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.mlflow:mlflow-spark:2.5.0\") \\\n",
    "        .getOrCreate()\n",
    "print(\"Spark Version: \", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2024/12/30 21:57:01 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logged successfully!\n",
      "üèÉ View run zealous-auk-814 at: http://localhost:5000/#/experiments/805412585363482067/runs/c864456f969c4992b4aa730227460f4b\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/805412585363482067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 57642)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "       ^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.spark\n",
    "\n",
    "# Enable MLflow tracking\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")  # Set your MLflow tracking server URI\n",
    "mlflow.set_experiment(\"PySpark_MLflow_Experiment\")\n",
    "\n",
    "# Example PySpark workflow with MLflow\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Sample data\n",
    "data = [(0, 1.0, 2.0, 3.0), (1, 2.0, 3.5, 4.0), (0, 3.0, 4.0, 5.0)]\n",
    "columns = [\"label\", \"feature1\", \"feature2\", \"feature3\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Assemble features\n",
    "assembler = VectorAssembler(inputCols=[\"feature1\", \"feature2\", \"feature3\"], outputCol=\"features\")\n",
    "df = assembler.transform(df)\n",
    "\n",
    "# Train a logistic regression model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Start MLflow run\n",
    "with mlflow.start_run():\n",
    "    model = lr.fit(df)\n",
    "    mlflow.spark.log_model(model, \"logistic_regression_model\")  # Log the PySpark MLlib model\n",
    "    mlflow.log_param(\"maxIter\", lr.getMaxIter())  # Log a parameter\n",
    "    mlflow.log_metric(\"training_accuracy\", 0.9)  # Log a metric\n",
    "\n",
    "    print(\"Model logged successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
