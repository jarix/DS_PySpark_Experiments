{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello MLLib simple DecisionTreeClassifier  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prereqquisites\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version:  3.4.1\n"
     ]
    }
   ],
   "source": [
    "# Spark Session and Context\n",
    "spark = SparkSession.builder.master(\"local\") \\\n",
    "        .appName(\"Simple MLlib DecisionTreeClassifier\") \\\n",
    "        .getOrCreate()\n",
    "print(\"Spark Version: \", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+-----+\n",
      "| id|feature1|feature2|label|\n",
      "+---+--------+--------+-----+\n",
      "|  0|     1.0|     2.0|    0|\n",
      "|  1|     2.0|     3.0|    1|\n",
      "|  2|     3.0|     4.0|    0|\n",
      "|  3|     4.0|     5.0|    1|\n",
      "|  4|     5.0|     6.0|    0|\n",
      "+---+--------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Make up some sample data\n",
    "data = [\n",
    "    (0, 1.0, 2.0, 0),\n",
    "    (1, 2.0, 3.0, 1),\n",
    "    (2, 3.0, 4.0, 0),\n",
    "    (3, 4.0, 5.0, 1),\n",
    "    (4, 5.0, 6.0, 0),\n",
    "]\n",
    "columns = [\"id\", \"feature1\", \"feature2\", \"label\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+-----+---------+-------------+-----------+----------+\n",
      "| id|feature1|feature2|label| features|rawPrediction|probability|prediction|\n",
      "+---+--------+--------+-----+---------+-------------+-----------+----------+\n",
      "|  2|     3.0|     4.0|    0|[3.0,4.0]|    [0.0,2.0]|  [0.0,1.0]|       1.0|\n",
      "+---+--------+--------+-----+---------+-------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "\n",
    "# Assemble Features\n",
    "assembler = VectorAssembler(inputCols=[\"feature1\", \"feature2\"], outputCol=\"features\")\n",
    "df_transformed = assembler.transform(df)\n",
    "\n",
    "# Split data into Train and Test datasets\n",
    "train_data, test_data = df_transformed.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Initialize and Train DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"label\", maxDepth=3)\n",
    "model = dt.fit(train_data)\n",
    "\n",
    "# Step 6: Make Predictions\n",
    "predictions = model.transform(test_data)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n",
      "+---+---------+-----+----------+-----------+\n",
      "| id| features|label|prediction|probability|\n",
      "+---+---------+-----+----------+-----------+\n",
      "|  2|[3.0,4.0]|    0|       1.0|  [0.0,1.0]|\n",
      "+---+---------+-----+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Evaluate the Model\n",
    "# Evaluate accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Evaluate precision, recall, and F1-score\n",
    "precision_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\"\n",
    ")\n",
    "precision = precision_evaluator.evaluate(predictions)\n",
    "print(f\"Precision: {precision}\")\n",
    "\n",
    "recall_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\"\n",
    ")\n",
    "recall = recall_evaluator.evaluate(predictions)\n",
    "print(f\"Recall: {recall}\")\n",
    "\n",
    "f1_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    ")\n",
    "f1 = f1_evaluator.evaluate(predictions)\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# Inspect Predictions\n",
    "predictions.select(\"id\", \"features\", \"label\", \"prediction\", \"probability\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
