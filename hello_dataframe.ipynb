{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78a708fa",
   "metadata": {},
   "source": [
    "# Spark DataFrame samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338f99f6",
   "metadata": {},
   "source": [
    "Python API Documentation:  https://spark.apache.org/docs/latest/api/python/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcfb857c-2739-4a3a-810e-e42e920d625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prereqquisites\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b02f09f-9f48-4b4e-bfea-9a5843a67381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version:  3.5.0\n"
     ]
    }
   ],
   "source": [
    "# Spark Session and Context\n",
    "spark = SparkSession.builder.master(\"local\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "print(\"Spark Version: \", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7257ecf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version:  1.24.4\n",
      "Pandas version:  2.0.3\n",
      "Matplotlib version:  3.8.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "print(\"NumPy version: \", np.__version__)\n",
    "print(\"Pandas version: \", pd.__version__)\n",
    "print(\"Matplotlib version: \", plt.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756c5de4",
   "metadata": {},
   "source": [
    "### Load dataframe from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cff5d8b1-7e22-4265-b337-49ce47f9969a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the data file within the \"sparkdata\" volume\n",
    "data_path = \"data/taxi_zone_lookup.csv\"\n",
    "\n",
    "# Load the data into a PySpark DataFrame\n",
    "#df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Alternative Way to read:\n",
    "# Default format is Parquet, if .format() not specified\n",
    "\"\"\"\n",
    "df = (spark.read\n",
    "      .format(\"csv\")\n",
    "      .option(\"HEADER\", \"true\")\n",
    "      .option(\"InferSchEMA\" ,\"true\")\n",
    "      .load(data_path))\n",
    "\"\"\"\n",
    "\n",
    "# Another Alternative Way to read:\n",
    "df = (spark.read\n",
    "      .format(\"csv\")\n",
    "      .option(\"HEADER\", \"true\")\n",
    "      .option(\"InferSchEMA\" ,\"true\")\n",
    "      .option(\"path\", \"data/taxi_zone_lookup.csv\")\n",
    "      .load())\n",
    "      \n",
    "# Show the first few rows of the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "386f64f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[LocationID: int, Borough: string, Zone: string, service_zone: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use the display function\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "477c7ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numer of rows:  265\n",
      "root\n",
      " |-- LocationID: integer (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Numer of rows: \", df.count())\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dfae26",
   "metadata": {},
   "source": [
    "## Create a Dataframe manually and group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c9c9a329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "|Brooke| 20|\n",
      "| Denny| 31|\n",
      "| Jules| 30|\n",
      "|    TD| 35|\n",
      "|Brooke| 25|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Dataframe\n",
    "df_persons = spark.createDataFrame([(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30), (\"TD\", 35), \n",
    "                                    (\"Brooke\", 25)], [\"name\", \"age\"])\n",
    "\n",
    "df_persons.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8b2b5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  name|avg(age)|\n",
      "+------+--------+\n",
      "|Brooke|    22.5|\n",
      "| Jules|    30.0|\n",
      "|    TD|    35.0|\n",
      "| Denny|    31.0|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group names and compute average\n",
    "df_ave = df_persons.groupBy(\"name\").agg(avg(\"age\"))\n",
    "df_ave.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bcbcff",
   "metadata": {},
   "source": [
    "## Create Dataframe with Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e239bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c576808d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Id` INT,`First` STRING,`Last` STRING,`Url` STRING,`Published` STRING,`Hits` INT,`Campaigns` ARRAY<STRING>\n"
     ]
    }
   ],
   "source": [
    "# Create Schema\n",
    "schema =  \"`Id` INT,`First` STRING,`Last` STRING,`Url` STRING,`Published` STRING,`Hits` INT,`Campaigns` ARRAY<STRING>\"\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e6c6cd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create static data\n",
    "data = [[1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\",\n",
    "\"LinkedIn\"]],\n",
    "       [2, \"Brooke\",\"Wenig\", \"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\",\n",
    "\"LinkedIn\"]],\n",
    "       [3, \"Denny\", \"Lee\", \"https://tinyurl.3\", \"6/7/2019\", 7659, [\"web\",\n",
    "\"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "       [4, \"Tathagata\", \"Das\", \"https://tinyurl.4\", \"5/12/2018\", 10568, \n",
    "[\"twitter\", \"FB\"]],\n",
    "       [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\",\n",
    "\"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "       [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568, \n",
    "[\"twitter\", \"LinkedIn\"]]\n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "35f43d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Dataframe\n",
    "df_blogs = spark.createDataFrame(data, schema)\n",
    "\n",
    "df_blogs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "193952b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema:\n",
      " StructType([StructField('Id', IntegerType(), True), StructField('First', StringType(), True), StructField('Last', StringType(), True), StructField('Url', StringType(), True), StructField('Published', StringType(), True), StructField('Hits', IntegerType(), True), StructField('Campaigns', ArrayType(StringType(), True), True)])\n"
     ]
    }
   ],
   "source": [
    "# Get Schema definition\n",
    "print(\"Schema:\\n\", df_blogs.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0febc8b",
   "metadata": {},
   "source": [
    "## Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7867585d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|(Hits * 2)|\n",
      "+----------+\n",
      "|      9070|\n",
      "|     17816|\n",
      "|     15318|\n",
      "|     21136|\n",
      "|     81156|\n",
      "|     51136|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select Hits Column and multiply by 2\n",
    "df_blogs.select(expr(\"Hits\") * 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "18ac11a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|(Id + Hits)|\n",
      "+-----------+\n",
      "|4536       |\n",
      "|8910       |\n",
      "|7662       |\n",
      "|10572      |\n",
      "|40583      |\n",
      "|25574      |\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add togeher Id and Hits columns\n",
    "df_blogs.select(expr(\"Id\") + expr(\"Hits\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4917612a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|Big Hitters|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|      false|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|      false|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|      false|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|      false|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|       true|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|       true|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a Column\n",
    "df_blogs.withColumn(\"Big Hitters\", (expr(\"Hits\") > 15000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "af593396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|    AuthorsId|\n",
      "+-------------+\n",
      "|  JulesDamji1|\n",
      "| BrookeWenig2|\n",
      "|    DennyLee3|\n",
      "|TathagataDas4|\n",
      "|MateiZaharia5|\n",
      "|  ReynoldXin6|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Concatenate columns\n",
    "df_blogs.withColumn(\"AuthorsId\", (concat(expr(\"First\"), expr(\"Last\"), expr(\"Id\")))).select(expr(\"AuthorsId\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1d4deadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Original\n",
    "df_blogs.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8cca50",
   "metadata": {},
   "source": [
    "## Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "59952d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dc8da5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xin\n"
     ]
    }
   ],
   "source": [
    "# Create a Row object\n",
    "blog_row = Row(6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", 255568, \"3/2/2015\", [\"twitter\", \"LinkedIn\"])\n",
    "# Access using element with 0-counted index\n",
    "print(blog_row[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ce0fcf05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+---+\n",
      "|   Authors|State|Age|\n",
      "+----------+-----+---+\n",
      "|John Smith|   WA| 36|\n",
      "|  Jane Doe|   CA| 62|\n",
      "+----------+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a datafame with ROws\n",
    "rows = [Row(\"John Smith\", \"WA\", 36), Row(\"Jane Doe\", \"CA\", 62)]\n",
    "df_people = spark.createDataFrame(rows, [\"Authors\", \"State\", \"Age\"])\n",
    "df_people.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5f4d4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
